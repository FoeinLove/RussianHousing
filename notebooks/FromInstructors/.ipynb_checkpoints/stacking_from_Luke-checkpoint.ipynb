{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stacking\n",
    "\n",
    "This notebook demonstrates the stacking technique with the data in <a href=https://www.kaggle.com/c/two-sigma-connect-rental-listing-inquiries>Two Sigma Connect: Rental Listing Inquiries</a> from <a href=https://www.kaggle.com/competitions>Kaggle competition</a>. Reading the detail is left to the audience, but to summarise:\n",
    "\n",
    "- The data is from <a href=https://www.renthop.com/>RentHop</a>, which is a web and mobile-based search engine that allows users to search for apartments in major cities.\n",
    "- Base on the data for each listing the purpose is to predict the interest level of each listing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the data\n",
    "\n",
    "The data from Kaggle comes with the training data and the test data, the latter has no correct label. For the purpose of demonstration, we will use only the training data. The test data is loaded though.\n",
    "\n",
    "- Below we print out the columns from the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bathrooms\n",
      "bedrooms\n",
      "building_id\n",
      "created\n",
      "description\n",
      "display_address\n",
      "features\n",
      "interest_level\n",
      "latitude\n",
      "listing_id\n",
      "longitude\n",
      "manager_id\n",
      "photos\n",
      "price\n",
      "street_address\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import time\n",
    "\n",
    "with open('./data/train.json') as data_file:    \n",
    "    data = json.load(data_file)\n",
    "\n",
    "raw_train = pd.DataFrame(data)\n",
    "\n",
    "with open('./data/test.json') as data_file:    \n",
    "    data = json.load(data_file)\n",
    "\n",
    "raw_test = pd.DataFrame(data)\n",
    "\n",
    "for item in raw_train.columns:\n",
    "    print item"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The id is used for identifying the listing in the competition. For simplicity, we reindex the data by integers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "_id = raw_train.index\n",
    "raw_train=raw_train.reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to train a popular model: <a href=https://github.com/dmlc/xgboost>xgboost</a>. We will start with only the numerical feature:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "col = ['bathrooms', 'bedrooms', 'latitude', 'longitude', 'price']\n",
    "X = raw_train[col]\n",
    "y = raw_train['interest_level'].apply(lambda x: 0 if x=='low' else 1 if x=='medium' else 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Folding \n",
    "\n",
    "To be able to evaluate our model, we use the package `model_selection` to split the indices into:\n",
    "\n",
    "- Training data (used to train the models)\n",
    "- Validation data (used to evaluate the models. We avoid using the term \"test data\" to differentiate this from the test data from Kaggle)\n",
    "\n",
    "For the later usage in stacking, we also split the training data into two folds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sklearn\n",
    "from sklearn import model_selection\n",
    "\n",
    "skf = model_selection.StratifiedKFold(n_splits=3)\n",
    "folds = skf.split(X, y)\n",
    "_, fold1 = folds.next()\n",
    "_, fold2 = folds.next()\n",
    "_, validation_idx = folds.next()\n",
    "\n",
    "train_idx = np.concatenate([fold1, fold2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we initialize the parameters for xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "\n",
    "param = {}\n",
    "param['objective'] = 'multi:softprob'\n",
    "param['eta'] = 0.02\n",
    "param['max_depth'] = 6\n",
    "param['silent'] = 1\n",
    "param['num_class'] = 3\n",
    "param['eval_metric'] = \"mlogloss\"\n",
    "param['min_child_weight'] = 3\n",
    "param['subsample'] = 0.7\n",
    "param['colsample_bytree'] = 0.7\n",
    "param['seed'] = 321\n",
    "num_rounds = 1000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we train and evaluate the xgboost model. We use the logloss as the metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The log loss is: 0.656\n",
      "Time elapsed: 36.94 seconds\n"
     ]
    }
   ],
   "source": [
    "start_ = time.time()\n",
    "X_train = np.array(X.iloc[train_idx])\n",
    "y_train = np.array(y.iloc[train_idx])\n",
    "xgtrain = xgb.DMatrix(X_train, label= y_train)\n",
    "\n",
    "clf = xgb.train(param, xgtrain, num_rounds)\n",
    "\n",
    "X_validation = np.array(X.iloc[validation_idx])\n",
    "y_validation = np.array(y.iloc[validation_idx])\n",
    "xgvalidation = xgb.DMatrix(X_validation)\n",
    "y_prob = clf.predict(xgvalidation)\n",
    "\n",
    "print 'The log loss is: %.3f' % sklearn.metrics.log_loss(y_validation, y_prob)\n",
    "print 'Time elapsed: %.2f seconds' % (time.time() - start_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding the text data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean the text\n",
    "\n",
    "The code below cleans the \"description\" column in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time elapsed: 441.20\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import nltk\n",
    "from nltk.tag import pos_tag\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "stop = stopwords.words('english')\n",
    "\n",
    "def cleaning_text(sentence):\n",
    "    sentence = sentence.encode('ascii', errors='replace')\n",
    "    sentence=sentence.lower()\n",
    "    sentence=re.sub('[^\\w\\s]',' ', sentence) #removes punctuations\n",
    "    sentence=re.sub('_', ' ', sentence) #removes punctuations\n",
    "    sentence=re.sub('\\d+',' ', sentence) #removes digits\n",
    "    cleaned=' '.join([w for w in sentence.split() if not w in stop]) \n",
    "    #removes english stopwords\n",
    "    cleaned=' '.join([w for w , pos in pos_tag(cleaned.split()) if (pos == 'NN' or pos=='JJ' or pos=='JJR' or pos=='JJS' )])\n",
    "    #selecting only nouns and adjectives\n",
    "    \n",
    "    cleaned=' '.join([w for w in cleaned.split() if not len(w)<=2 ]) \n",
    "    #removes single lettered words and digits\n",
    "    cleaned=cleaned.strip()\n",
    "    return cleaned\n",
    "\n",
    "start_ = time.time()\n",
    "raw_train['cleaned_txt'] = raw_train['description'].apply(lambda x: cleaning_text(x))\n",
    "print 'Time elapsed: %.2f' % (time.time()-start_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF\n",
    "\n",
    "To take the text data into account, we need to somehow encoding the chategorical features into numerical features. Dummification (or one-hot encoding) is a popular way for general chategorical features. For text data, we would use a much more efficient method: <a gref=https://en.wikipedia.org/wiki/Tf%E2%80%93idf>tf-idf</a>. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "n_features = 500\n",
    "n_topics = 10\n",
    "n_top_words = 20\n",
    "\n",
    "tf_vectorizer = CountVectorizer(max_df=0.95, min_df=2, max_features=n_features,\n",
    "                                stop_words='english')\n",
    "\n",
    "text_train = np.array(raw_train.loc[train_idx, 'cleaned_txt'].replace(np.nan, ''))\n",
    "tf = tf_vectorizer.fit_transform(text_train)\n",
    "TF = TfidfTransformer()\n",
    "tf_idf_train = TF.fit_transform(tf)\n",
    "tf_idf_train = tf_idf_train.toarray()\n",
    "X_train_des = np.concatenate([X_train, tf_idf_train], axis=1)\n",
    "\n",
    "\n",
    "text_validation = np.array(raw_train.loc[validation_idx, 'cleaned_txt'].replace(np.nan, ''))\n",
    "tf = tf_vectorizer.transform(text_validation)\n",
    "tf_idf_validation = TF.transform(tf)\n",
    "tf_idf_validation = tf_idf_validation.toarray()\n",
    "X_validation_des = np.concatenate([X_validation, tf_idf_validation], axis=1)\n",
    "\n",
    "text_fold1 = np.array(raw_train.loc[fold1, 'cleaned_txt'].replace(np.nan, ''))\n",
    "text_fold2 = np.array(raw_train.loc[fold2, 'cleaned_txt'].replace(np.nan, ''))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we trained the model with both the numerical feature we used before, and the new text data.\n",
    "\n",
    "We see that the performance is improved with no surprise -- we took more feature into account. However, the improvement takes a lot of time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time elapsed: 2165.19 seconds\n",
      "The log loss is: 0.628\n"
     ]
    }
   ],
   "source": [
    "start_ = time.time()\n",
    "xgtrain_des = xgb.DMatrix(X_train_des, label= y_train)\n",
    "clf = xgb.train(param, xgtrain_des, num_rounds)\n",
    "xgvalidation_des = xgb.DMatrix(X_validation_des)\n",
    "y_prob_des = clf.predict(xgvalidation_des)\n",
    "print 'Time elapsed: %.2f seconds' % (time.time() - start_)\n",
    "print 'The log loss is: %.3f' % sklearn.metrics.log_loss(y_validation, y_prob_des)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stacking models with resampling\n",
    "\n",
    "In the demo above we saw a drawback of the tree-based models: inefficiency when dealing with large cardinality of a chategorical feature.\n",
    "\n",
    "One thing we can do is to train some simpler models first on the text data, and then stack back with the numerical predictors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using sparse matrix\n",
    "\n",
    "The text data contains a lot of zeros, one simple way to gain efficiency is to use the sparse matrices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf = tf_vectorizer.transform(text_train)\n",
    "tf_idf_train = TF.transform(tf)\n",
    "\n",
    "tf = tf_vectorizer.transform(text_fold1)\n",
    "tf_idf_fold1 = TF.transform(tf)\n",
    "\n",
    "tf = tf_vectorizer.transform(text_fold2)\n",
    "tf_idf_fold2 = TF.transform(tf)\n",
    "\n",
    "tf = tf_vectorizer.transform(text_validation)\n",
    "tf_idf_validation = TF.transform(tf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the new feature\n",
    "\n",
    "The function below create the new predictors with 2 folds as we described in the slides."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_fold1 = np.array(y.iloc[fold1])\n",
    "y_fold2 = np.array(y.iloc[fold2])\n",
    "\n",
    "def get_2fold_stack(model):\n",
    "    model.fit(tf_idf_fold1, y_fold1)\n",
    "    new_fold2 = model.predict_proba(tf_idf_fold2)[:,:2]\n",
    "    v1 = model.predict_proba(tf_idf_validation)[:,:2]  ### There is model\n",
    "    model.fit(tf_idf_fold2, y_fold2)\n",
    "    new_fold1 = model.predict_proba(tf_idf_fold1)[:,:2]\n",
    "    v2 = model.predict_proba(tf_idf_validation) [:,:2]### The model here is different\n",
    "\n",
    "    return np.concatenate([new_fold1, new_fold2], axis=0), (v1+v2)/2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we add the predictor created by logistic regression. The performance is actually much better than training with only numerical predictors, even though the time required is not that much longer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The log loss is: 0.641\n",
      "Time elapsed: 52.21 seconds\n"
     ]
    }
   ],
   "source": [
    "start_ = time.time()\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "logit = LogisticRegression()\n",
    "\n",
    "f_logit_train, f_logit_validation = get_2fold_stack(logit)\n",
    "\n",
    "X_train_stack = np.concatenate([X_train, f_logit_train], axis=1)\n",
    "xgtrain_stack = xgb.DMatrix(X_train_stack, label= y_train)\n",
    "X_validation_stack = np.concatenate([X_validation, f_logit_validation], axis=1)\n",
    "xgvalidation_stack = xgb.DMatrix(X_validation_stack)\n",
    "\n",
    "clf = xgb.train(param, xgtrain_stack, num_rounds)\n",
    "y_prob_stack = clf.predict(xgvalidation_stack)\n",
    "print 'The log loss is: %.3f' % sklearn.metrics.log_loss(y_validation, y_prob_stack)\n",
    "print 'Time elapsed: %.2f seconds' % (time.time() - start_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stacking multiple models\n",
    "\n",
    "We may of course stack more than just one model. Below we stack:\n",
    "\n",
    "- logistic regression (with different penalty constant)\n",
    "- knn (with different k)\n",
    "- latenet dirichlet allocation (folding is not required since this is an unsupervised learning)\n",
    "- naive bayes and quadratic discriminant analysis (densed matrices are required so the rasampling function is rewritten).\n",
    "\n",
    "We see that the stacking model takes only around one-third of the time to reach the same level of performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/lukelin/anaconda2/lib/python2.7/site-packages/sklearn/decomposition/online_lda.py:508: DeprecationWarning: The default value for 'learning_method' will be changed from 'online' to 'batch' in the release 0.20. This warning was introduced in 0.18.\n",
      "  DeprecationWarning)\n",
      "/Users/lukelin/anaconda2/lib/python2.7/site-packages/sklearn/discriminant_analysis.py:695: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The log loss is: 0.626\n",
      "Time elapsed: 605.73 seconds\n"
     ]
    }
   ],
   "source": [
    "start_ = time.time()\n",
    "\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "logit1000 = LogisticRegression(C=1000)\n",
    "f_logit1000_train, f_logit1000_validation = get_2fold_stack(logit1000)\n",
    "\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn10 = KNeighborsClassifier(n_neighbors=10)\n",
    "f_knn10_train, f_knn10_validation = get_2fold_stack(knn10)\n",
    "\n",
    "knn100 = KNeighborsClassifier(n_neighbors=100)\n",
    "f_knn100_train, f_knn100_validation = get_2fold_stack(knn100)\n",
    "\n",
    "\n",
    "knn10000 = KNeighborsClassifier(n_neighbors=10000)\n",
    "f_knn10000_train, f_knn10000_validation = get_2fold_stack(knn10000)\n",
    "\n",
    "\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "topic10 = LatentDirichletAllocation(n_topics=10)\n",
    "f_topics10_train = topic10.fit_transform(tf_idf_train)\n",
    "f_topics10_validation = topic10.transform(tf_idf_validation)\n",
    "\n",
    "\n",
    "def get_2fold_stack_dense(model):\n",
    "    _f1 = tf_idf_fold1.toarray()\n",
    "    _f2 = tf_idf_fold2.toarray()\n",
    "    _v = tf_idf_validation.toarray()\n",
    "    model.fit(_f1, y_fold1)\n",
    "    new_fold2 = model.predict_proba(_f2)[:,:2]\n",
    "    v1 = model.predict_proba(_v)[:,:2]  ### There is model\n",
    "    model.fit(_f2, y_fold2)\n",
    "    new_fold1 = model.predict_proba(_f1)[:,:2]\n",
    "    v2 = model.predict_proba(_v) [:,:2]### The model here is different\n",
    "\n",
    "    return np.concatenate([new_fold1, new_fold2], axis=0), (v1+v2)/2\n",
    "\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "bnb = BernoulliNB(binarize=0.00001)\n",
    "f_bnb_train, f_bnb_validation = get_2fold_stack(bnb)\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "mnb = MultinomialNB()\n",
    "f_mnb_train, f_mnb_validation = get_2fold_stack(mnb)\n",
    "\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "qda = QuadraticDiscriminantAnalysis()\n",
    "f_qda_train, f_qda_validation = get_2fold_stack_dense(qda)\n",
    "\n",
    "\n",
    "X_train_stack = np.concatenate([X_train, \n",
    "                                f_logit_train,\n",
    "                                f_logit1000_train,\n",
    "                                f_knn10_train,\n",
    "                                f_knn100_train,  \n",
    "                                f_knn10000_train,\n",
    "                                f_mnb_train, \n",
    "                                f_bnb_train,\n",
    "                                f_qda_train,\n",
    "                                f_topics10_train], axis=1)\n",
    "xgtrain_stack = xgb.DMatrix(X_train_stack, label= y_train)\n",
    "\n",
    "\n",
    "X_validation_stack = np.concatenate([X_validation, \n",
    "                                     f_logit_validation,\n",
    "                                     f_logit1000_validation,\n",
    "                                     f_knn10_validation,\n",
    "                                     f_knn100_validation, \n",
    "                                     f_knn10000_validation, \n",
    "                                     f_mnb_validation,\n",
    "                                     f_bnb_validation,\n",
    "                                     f_qda_validation,\n",
    "                                     f_topics10_validation], axis=1)\n",
    "xgvalidation_stack = xgb.DMatrix(X_validation_stack)\n",
    "\n",
    "clf = xgb.train(param, xgtrain_stack, num_rounds)\n",
    "y_prob_stack = clf.predict(xgvalidation_stack)\n",
    "print 'The log loss is: %.3f' % sklearn.metrics.log_loss(y_validation, y_prob_stack)\n",
    "print 'Time elapsed: %.2f seconds' % (time.time() - start_)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
