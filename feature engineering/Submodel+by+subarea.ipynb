{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "from sklearn import model_selection, preprocessing\n",
    "import xgboost as xgb\n",
    "from datetime import timedelta\n",
    "import datetime\n",
    "import math\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/williamzhou/Downloads/RussianHousing-master\n"
     ]
    }
   ],
   "source": [
    "cd /Users/williamzhou/Downloads/RussianHousing-master"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('train shape', (30471, 300))\n",
      "('test shape', (7662, 297))\n",
      "('feature_importance shape', (276, 3))\n"
     ]
    }
   ],
   "source": [
    "train=pd.read_csv('./data/processed/Clean0517/train_clean_shu_0517.csv')\n",
    "test = pd.read_csv('./data/processed/Clean0517/test_clean_shu_0517.csv')\n",
    "macro = pd.read_csv('./data/raw/macro.csv')\n",
    "# latlon = pd.read_csv('./data/external/sub_area_lon_lat.csv')\n",
    "feature_importance = pd.read_csv('/Users/williamzhou/Documents/github/RussianHousing/feature engineering/feature_importance.csv')\n",
    "\n",
    "train['timestamp'] = pd.to_datetime(train['timestamp'])\n",
    "macro['timestamp'] = pd.to_datetime(macro['timestamp'])\n",
    "test['timestamp'] = pd.to_datetime(test['timestamp'])\n",
    "\n",
    "train['label']='train'\n",
    "test['label']='test'\n",
    "\n",
    "print ('train shape',train.shape)\n",
    "print ('test shape',test.shape)\n",
    "print ('feature_importance shape', feature_importance.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Merge with Macro data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('data shape:', (30471, 302))\n",
      "('data shape:', (7662, 299))\n"
     ]
    }
   ],
   "source": [
    "time_laps_dict ={\n",
    "#                     'usdrub':0#,\n",
    "                 'micex_cbi_tr':0,\n",
    "                 'micex_rgbi_tr':0#,\n",
    "#                  'deposits_value':30\n",
    "#                  'ppi':90,\n",
    "#                  'cpi':300\n",
    "                }\n",
    "\n",
    "def merge_macro_feature(dicts,macro,df):\n",
    "    \n",
    "    for item in dicts:\n",
    "        macro_timeshift=macro.copy()\n",
    "        macro_timeshift['timestamp']=macro.timestamp+timedelta(days=dicts[item])\n",
    "        df = pd.merge(df,macro_timeshift[['timestamp',item]],on='timestamp',how='left')\n",
    "    print('data shape:',df.shape)\n",
    "    return(df)\n",
    "\n",
    "train =merge_macro_feature(time_laps_dict,macro,train)\n",
    "test=merge_macro_feature(time_laps_dict,macro,test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "subarea are ['other' 'Poselenie Sosenskoe']\n"
     ]
    }
   ],
   "source": [
    "#### Group frequent area\n",
    "freq_area = np.array(train.loc[:, 'sub_area'].value_counts()[:2].index)\n",
    "train.loc[~train['sub_area'].isin(freq_area), 'sub_area'] = 'other'\n",
    "test.loc[~test['sub_area'].isin(freq_area), 'sub_area'] = 'other'\n",
    "print ('subarea are {}'.format(train.sub_area.unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('train shape', (30471, 303))\n",
      "('test shape', (7662, 299))\n"
     ]
    }
   ],
   "source": [
    "train.loc[train['full_sq'].isnull(),'full_sq']=train['full_sq'].median()\n",
    "test.loc[test['full_sq'].isnull(),'full_sq']=test['full_sq'].median()\n",
    "train['price_full_sq']=train['price_doc']/train['full_sq']\n",
    "train['price_full_sq']=train['price_full_sq'].astype('int64')\n",
    "\n",
    "# train= median_price_sqm_by_ID('ID_metro',train)\n",
    "# test= median_price_sqm_by_ID('ID_metro',test)\n",
    "print ('train shape',train.shape)\n",
    "print ('test shape',test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('x_train shape:', (30471, 301))\n",
      "('y_train shape:', (30471, 2))\n",
      "('x_test shape:', (7662, 298))\n",
      "('test_id shape:', (7662, 2))\n"
     ]
    }
   ],
   "source": [
    "y_train=train[['price_full_sq','sub_area']]\n",
    "x_train= train.drop(['price_doc','price_full_sq'],axis=1)\n",
    "test_id = test[['id','sub_area']]\n",
    "x_test = test.drop(['id'],axis=1)\n",
    "\n",
    "print('x_train shape:',x_train.shape)\n",
    "print('y_train shape:',y_train.shape)\n",
    "print('x_test shape:',x_test.shape)\n",
    "print('test_id shape:',test_id.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_all = pd.concat([x_train,x_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(30471, 301)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train=df_all.loc[df_all.label=='train',:]\n",
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('x_train shape', (30471, 302))\n",
      "('x_test shape', (7662, 302))\n",
      "('y_train shape', (30471, 2))\n"
     ]
    }
   ],
   "source": [
    "IDs = ['ID_big_road1','ID_big_road2',\n",
    "        'ID_bus_terminal','ID_metro',\n",
    "       'ID_railroad_station_avto',\n",
    "       'ID_railroad_terminal']\n",
    "\n",
    "def to_object(ID,df):\n",
    "    df[ID] = df[ID].astype(object)\n",
    "    return(df)\n",
    "\n",
    "df_all=to_object(IDs,df_all)\n",
    "\n",
    "    \n",
    "month_year = (df_all.timestamp.dt.month + df_all.timestamp.dt.year * 100)\n",
    "month_year_cnt_map = month_year.value_counts().to_dict()\n",
    "df_all['month_year_cnt'] = month_year.map(month_year_cnt_map)\n",
    "\n",
    "# Add week-year count\n",
    "week_year = (df_all.timestamp.dt.weekofyear + df_all.timestamp.dt.year * 100)\n",
    "week_year_cnt_map = week_year.value_counts().to_dict()\n",
    "df_all['week_year_cnt'] = week_year.map(week_year_cnt_map)\n",
    "\n",
    "for c in df_all.columns:\n",
    "    if df_all[c].dtype == 'object' and c!='label' and c!='sub_area':\n",
    "        lbl = preprocessing.LabelEncoder()\n",
    "        lbl.fit(list(df_all[c].values)) \n",
    "        df_all[c] = lbl.transform(list(df_all[c].values))\n",
    "\n",
    "df_all=df_all.drop(['timestamp'],axis =1 )\n",
    "\n",
    "x_train = df_all.loc[df_all.label=='train',:]\n",
    "x_test = df_all.loc[df_all.label=='test',:]\n",
    "\n",
    "print('x_train shape',x_train.shape)\n",
    "print('x_test shape',x_test.shape)\n",
    "print('y_train shape',y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training XGBoost with 40 features\n"
     ]
    }
   ],
   "source": [
    "final_features = pd.read_csv('./cv_output/final_feature.csv')\n",
    "train_feature = list(final_features.iloc[:,0])\n",
    "print('Training XGBoost with %d features'%len(train_feature))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subareas has 2 areas\n"
     ]
    }
   ],
   "source": [
    "subareas=x_train.sub_area.unique()\n",
    "print('Subareas has %d areas'%len(subareas))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "xgb_params = {\n",
    "    'eta': 0.05,\n",
    "    'max_depth': 5,\n",
    "    'subsample': 0.7,\n",
    "    'colsample_bytree': 0.7,\n",
    "    'objective': 'reg:linear',\n",
    "    'eval_metric': 'rmse',\n",
    "    'silent': 0\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def median_missing_imputation(df):\n",
    "    for cols in df.columns:\n",
    "            df.loc[pd.isnull(df[cols]),cols] = np.median(df.loc[pd.isnull(df[cols])==False,cols])\n",
    "\n",
    "    return(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finish missing imputation.\n",
      "other x_train_subset shape (28695, 40)\n",
      "other y_test_subset shape (28695,)\n",
      "other x_test_subset shape (7365, 40)\n",
      "other test_id_subset shape (7365,)\n",
      "Ready! Start training other......\n",
      "[0]\ttrain-rmse:140001\ttest-rmse:140009\n",
      "[50]\ttrain-rmse:43521.4\ttest-rmse:45230.2\n",
      "[100]\ttrain-rmse:39923.7\ttest-rmse:42714.3\n",
      "[150]\ttrain-rmse:38309.1\ttest-rmse:42143.6\n",
      "[200]\ttrain-rmse:37024.7\ttest-rmse:41857.6\n",
      "[250]\ttrain-rmse:35942.8\ttest-rmse:41678.9\n",
      "[300]\ttrain-rmse:35035.8\ttest-rmse:41594.6\n",
      "[350]\ttrain-rmse:34166\ttest-rmse:41524.9\n",
      "[400]\ttrain-rmse:33324.1\ttest-rmse:41505.5\n",
      "other.csv successfully saved!\n",
      "Best number of iteration for other model: 393)\n",
      "Finish XGBoost training\n",
      "Predicting........\n",
      "['id', 'price_doc']\n",
      "Successfully trained other\n",
      "----------------------------------------------------------\n",
      "----------------------------------------------------------\n",
      "Finish missing imputation.\n",
      "Poselenie Sosenskoe x_train_subset shape (1776, 40)\n",
      "Poselenie Sosenskoe y_test_subset shape (1776,)\n",
      "Poselenie Sosenskoe x_test_subset shape (297, 40)\n",
      "Poselenie Sosenskoe test_id_subset shape (297,)\n",
      "Ready! Start training Poselenie Sosenskoe......\n",
      "[0]\ttrain-rmse:91498.5\ttest-rmse:91521.3\n",
      "[50]\ttrain-rmse:11506.7\ttest-rmse:14679\n",
      "[100]\ttrain-rmse:6570.7\ttest-rmse:12132.4\n",
      "[150]\ttrain-rmse:5463.28\ttest-rmse:11987.7\n",
      "[200]\ttrain-rmse:4692.48\ttest-rmse:11937\n",
      "[250]\ttrain-rmse:4091.95\ttest-rmse:11896.4\n",
      "[300]\ttrain-rmse:3590.76\ttest-rmse:11877.8\n",
      "[350]\ttrain-rmse:3179.15\ttest-rmse:11844.8\n",
      "Poselenie Sosenskoe.csv successfully saved!\n",
      "Best number of iteration for Poselenie Sosenskoe model: 377)\n",
      "Finish XGBoost training\n",
      "Predicting........\n",
      "['id', 'price_doc']\n",
      "Successfully trained Poselenie Sosenskoe\n",
      "----------------------------------------------------------\n",
      "----------------------------------------------------------\n",
      "The prediction dataset should have 7662 rows of data, your dataset has 7662 rows of data\n",
      "Successfully saved submission file! \n",
      "CPU times: user 1min 33s, sys: 590 ms, total: 1min 34s\n",
      "Wall time: 1min 35s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "sub=pd.DataFrame()\n",
    "for area in subareas:\n",
    "# for area in ['Strogino']:\n",
    "    x_train_subset = x_train.loc[x_train.sub_area==area,train_feature].reset_index(drop=True)\n",
    "    x_test_subset = x_test.loc[x_test.sub_area==area,train_feature].reset_index(drop=True)\n",
    "    y_train_subset = y_train.loc[x_train.sub_area==area,'price_full_sq'].reset_index(drop=True)\n",
    "    test_id_subset =test_id.loc[test_id.sub_area==area,'id'].reset_index(drop=True)\n",
    "    \n",
    "#     missing imputation using on subarea median\n",
    "#     x_train_subset = median_missing_imputation(x_train_subset)\n",
    "#     x_test_subset = median_missing_imputation(x_test_subset)\n",
    "    print('Finish missing imputation.')\n",
    "    dtrain_subset = xgb.DMatrix(x_train_subset, y_train_subset)\n",
    "    dtest_subset =  xgb.DMatrix(x_test_subset)\n",
    "    print('{} x_train_subset shape {}'.format(area,x_train_subset.shape))\n",
    "    print('{} y_test_subset shape {}'.format(area,y_train_subset.shape))\n",
    "    print('{} x_test_subset shape {}'.format(area,x_test_subset.shape))\n",
    "    print('{} test_id_subset shape {}'.format(area,test_id_subset.shape))\n",
    "    print('Ready! Start training {}......'.format(area))\n",
    "    \n",
    "    cv_output = xgb.cv(xgb_params, dtrain_subset, \n",
    "                   num_boost_round=1000, \n",
    "                   early_stopping_rounds=20,\n",
    "                   verbose_eval=50, show_stdv=False)\n",
    "    cv_output.to_csv('./train_by_subarea/{}.csv'.format(area))\n",
    "    print('{}.csv successfully saved!'.format(area))\n",
    "    num_boost_rounds = len(cv_output)\n",
    "    print('Best number of iteration for {} model: {})'.format(area,num_boost_rounds))\n",
    "    model = xgb.train(dict(xgb_params, silent=0), dtrain_subset, num_boost_round= num_boost_rounds)\n",
    "    print('Finish XGBoost training')\n",
    "    print('Predicting........')\n",
    "    y_predict = model.predict(dtest_subset)\n",
    "    y_predic_all_sq = (y_predict)*x_test_subset['full_sq']\n",
    "#     print(y_predic_all_sq)\n",
    "#     print(test_id.id[test_id.sub_area==area])\n",
    "    df_sub = pd.DataFrame({'id':test_id_subset , 'price_doc': y_predic_all_sq})\n",
    "    print([x for x in df_sub])\n",
    "    sub = pd.concat([sub,df_sub])\n",
    "    print('Successfully trained {}'.format(area))\n",
    "    print('----------------------------------------------------------')\n",
    "    print('----------------------------------------------------------')\n",
    "print('The prediction dataset should have 7662 rows of data, your dataset has {} rows of data'.format(len(sub)))\n",
    "\n",
    "sub.sort_values(by='id').reset_index(drop=True).to_csv('./train_by_subarea/sub_{}.csv'.format(time.ctime()),index=False)\n",
    "print('Successfully saved submission file! ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
